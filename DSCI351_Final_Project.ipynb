{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "08d0cd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import package\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from string import digits\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "import statistics\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c553ef66",
   "metadata": {},
   "source": [
    "## Load Data and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f97cea",
   "metadata": {},
   "source": [
    "### Load data from specific csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "5c8de45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename1='anime.csv', filename2='anime_with_synopsis.csv', filename3='rating_complete.csv'):\n",
    "    '''\n",
    "    This function is used to read the data and combine them.\n",
    "    :param filename1: data path\n",
    "    :param filename2: data path\n",
    "    :param filename3: data path\n",
    "    :return: Merged DataFrame\n",
    "    '''\n",
    "    df1=pd.read_csv(filename1)\n",
    "    df2=pd.read_csv(filename2)\n",
    "    df3=pd.read_csv(filename3)\n",
    "    new_df=df1.merge(df2, on=['MAL_ID','Genres'])\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df426648",
   "metadata": {},
   "source": [
    "### Preprocessing on raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "40f0a780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_UnknownName(x):\n",
    "    '''\n",
    "    This function is used to get all animes with English name \"Unknown\".\n",
    "    :param filename1: Raw DataFrame\n",
    "    :return: list of anime with English name 'Unknown'\n",
    "    '''\n",
    "    Unknown=[]\n",
    "    for i in range(x.shape[0]):\n",
    "        if x['English name'][i] == 'Unknown':\n",
    "            Unknown.append(i)\n",
    "        else:\n",
    "            continue\n",
    "    return Unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "056eae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_soup(x):\n",
    "    '''\n",
    "    This function is used to combine features 'Genres', 'Type' and 'Producers'.\n",
    "    :param x: sentence \n",
    "    :return: combined new feature 'soup'\n",
    "    '''\n",
    "    return  ' '.join(x['Genres']) + ',' + ' '.join(x['Type']) + ',' + ' '.join(x['Producers']) #+ str(x['sypnopsis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "e755f11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_words(x):\n",
    "    '''\n",
    "    This function is used to combine all features together.\n",
    "    :param x: merged data\n",
    "    :return: combined new feature 'word_features'\n",
    "    '''\n",
    "    return ''.join(x['soup']) + str(x['English name'])  + str(x['sypnopsis'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "89a446f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(x):\n",
    "    '''\n",
    "    This function is used to remove stopwords in merged features.\n",
    "    :param x: merged feature\n",
    "    :return: combined feature we will use 'word_features_tokenized'\n",
    "    '''\n",
    "    sentence_depart=word_tokenize(x)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    outstr = ''\n",
    "    for word in sentence_depart:\n",
    "        if word not in stop_words:\n",
    "            if word != '\\t':\n",
    "                outstr += word\n",
    "                outstr += \" \"\n",
    "    return outstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "35739628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(raw_data, features=['MAL_ID','English name','Genres','Type','Producers','sypnopsis'], user_id=64807):\n",
    "    '''\n",
    "    This function is used to do preprocessing work.\n",
    "    :param raw_data: combined data of csv file\n",
    "    :param features: features we will use in our recommender system\n",
    "    :param user_id: the target user id\n",
    "    :return: data after preprocessing\n",
    "    '''\n",
    "    data=raw_data[features]\n",
    "    \n",
    "    # Clean Raw Data \n",
    "    # Remove Unknown English name\n",
    "    for i in get_UnknownName(data):\n",
    "        data=data.drop(i,axis=0)\n",
    "    \n",
    "    # Lower all strings of selected features\n",
    "    features2=['English name','Genres','Type','Producers','sypnopsis']\n",
    "    for feature in features2:\n",
    "        data[feature]=data[feature].str.lower()\n",
    "    \n",
    "    # Remove space\n",
    "    data['Producers']=data['Producers'].str.replace(' ', '')\n",
    "    \n",
    "    #Delete all numbers, we only use words in our reconnendation system\n",
    "    table = str.maketrans('', '', digits)\n",
    "    data['sypnopsis']=data['sypnopsis'].str.translate(table)\n",
    "    \n",
    "    #Remove punctuation in feature 'sypnopsis'\n",
    "    try:\n",
    "        for i in list(data.index):\n",
    "            data['sypnopsis'][i]=re.sub('[\\W_]+', ' ', data['sypnopsis'][i])\n",
    "    except:\n",
    "        \"TypeError: expected string or bytes-like object\"\n",
    "    \n",
    "    #Apply function 'create_soup' to combine features 'Genres', 'Type' and 'Producers'\n",
    "    data['soup'] = data.apply(create_soup, axis=1)\n",
    "    data['soup']=data['soup'].str.replace(' ','')\n",
    "    \n",
    "    #Apply function 'bag_words' to combine all features together and get new column to store it\n",
    "    data['words_features'] = data.apply(bag_words, axis=1)\n",
    "    \n",
    "    #Remove all ',' i new features\n",
    "    data['words_features']=data['words_features'].str.replace(',',' ')\n",
    "    \n",
    "    #Apply function 'remove_stopwords' to remove stopwords in merged feature and get final merged feature\n",
    "    data['words_features_tokenized'] = data['words_features'].apply(lambda x : remove_stopwords(x))\n",
    "    \n",
    "    #select target user whose id is 64807 and add user's rating into current data to form new dataframe \n",
    "    df4=df3[df3['user_id']==user_id]\n",
    "    rating_df=data.merge(df4, left_on='MAL_ID', right_on='anime_id')\n",
    "    \n",
    "    return rating_df\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592e9255",
   "metadata": {},
   "source": [
    "## Building our system\n",
    "Here we use word2vec to calculate similarity matrix and implement our recommender system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eee526",
   "metadata": {},
   "source": [
    "### calculating similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "5df5cdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_similarity(data):\n",
    "    '''\n",
    "    This function is used to use word2vec to get similarity matrix for anime.\n",
    "    :param data: Processed data\n",
    "    :return: similarity matrix \n",
    "    '''\n",
    "    #get word vector\n",
    "    words = []\n",
    "    for num, vector in enumerate(rating_df['words_features_tokenized'].tolist()):\n",
    "        vector = [ str(x) for x in vector.split() ]\n",
    "#     document = TaggededDocument(title, tags=[num])\n",
    "        words.append(vector)\n",
    "    \n",
    "    model=Word2Vec(words,min_count=1,vector_size=100,window=6,workers=4,negative=5,sg=1)\n",
    "    \n",
    "    #get targets that will be applied to calculate cosine similarity\n",
    "    items = []\n",
    "    # for title in x_train_creative_id:\n",
    "    for num,vector in enumerate(rating_df['words_features_tokenized'].tolist()):\n",
    "        ss_product_id = []\n",
    "        vector1 = [ str(x) for x in vector.split() ]\n",
    "        for i in vector1:\n",
    "            ss_product_id.append(model.wv[str(i)])\n",
    "        ss = sum(ss_product_id)/len(ss_product_id)\n",
    "    #     print(type(model_dm.infer_vector(title)))\n",
    "        items.append(ss)\n",
    "        \n",
    "    #cosmine Similarity using word2vec, we call it w2v\n",
    "    #every anime we recommend top 10 similar animes\n",
    "    w2v = cosine_similarity(items)\n",
    "    for num,j in enumerate(w2v):\n",
    "        item = rating_df['English name'][num]\n",
    "        recs = []\n",
    "        for i in j.argsort()[::-1][1:10]:\n",
    "            recs.append(rating_df['English name'][i])\n",
    "    \n",
    "    return w2v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e62af8",
   "metadata": {},
   "source": [
    "### main funciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "40543a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_recommender(title, cosine_sim=w2v, data=rating_df):\n",
    "    '''\n",
    "    This function is used to use word2vec to make recommendations.\n",
    "    :param title: the input anime\n",
    "    :param cosine_sim: the similartiry matrix\n",
    "    :param data: Processed data\n",
    "    :return: top 10 related animes and their 'English name', 'Genres' and 'Rating'\n",
    "    '''\n",
    "    # get the index of the movie that matches the title\n",
    "    idx = data.index[data['English name']==title].tolist()[0]\n",
    "\n",
    "    # get the pairwsie similarity scores of all movies with input movie\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # sort the movies based on the cosine similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # get the scores of the 10 most similar movies (ignore the first movie).\n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    # get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # return the top 10 most similar movies\n",
    "    return data['English name'].iloc[movie_indices], data['rating'].iloc[movie_indices], data['Genres'].iloc[movie_indices] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "a793b5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(user_id=64807, title='great teacher onizuka', filename1='anime.csv', filename2='anime_with_synopsis.csv', filename3='rating_complete.csv'):\n",
    "    '''\n",
    "    This function is the main function of our recommender system.\n",
    "    :param user_id: the target user id\n",
    "    :param title: the input anime\n",
    "    :param filename1: data path\n",
    "    :param filename2: data path\n",
    "    :param filename3: data path\n",
    "    :return: top 10 related animes and their 'English name', 'Genres' and 'Rating'\n",
    "    '''\n",
    "    # read raw data\n",
    "    rating_df = read_data(filename1=filename1, filename2=filename2, filename3=filename3)\n",
    "    # preprocess raw data\n",
    "    processed_df = preprocessing(rating_df, user_id=user_id)\n",
    "    # calculate similarity matrix\n",
    "    w2v = word2vec_similarity(processed_df)\n",
    "    # make recommendations\n",
    "    name, rating, genre = content_recommender(title=title)\n",
    "    \n",
    "    return name, rating, genre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf550c2",
   "metadata": {},
   "source": [
    "### test on user 64807"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "6b2621a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "name, rating, genre = recommend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "1b7b21ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brother, dear brother \n",
      " 9 \n",
      " psychological, drama, school, shoujo, shoujo ai\n",
      "\n",
      "\n",
      "his and her circumstances \n",
      " 10 \n",
      " comedy, drama, romance, school, shoujo, slice of life\n",
      "\n",
      "\n",
      "haruka nogizaka's secret \n",
      " 7 \n",
      " comedy, romance\n",
      "\n",
      "\n",
      "miss machiko \n",
      " 7 \n",
      " comedy, ecchi, school\n",
      "\n",
      "\n",
      "ouran high school host club \n",
      " 8 \n",
      " comedy, harem, romance, school, shoujo\n",
      "\n",
      "\n",
      "chihayafuru \n",
      " 8 \n",
      " drama, game, josei, school, slice of life, sports\n",
      "\n",
      "\n",
      "tenjho tenge \n",
      " 9 \n",
      " action, ecchi, martial arts, comedy, super power, school, shounen\n",
      "\n",
      "\n",
      "encouragement of climb season 2 \n",
      " 6 \n",
      " adventure, comedy, slice of life\n",
      "\n",
      "\n",
      "citrus \n",
      " 6 \n",
      " drama, romance, school, shoujo ai\n",
      "\n",
      "\n",
      "attacker you! \n",
      " 6 \n",
      " action, romance, shoujo, sports\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(name.iloc[i],\"\\n\", rating.iloc[i],\"\\n\", genre.iloc[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "89ad6929",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rank padas to apply spearman\n",
    "new_series=pd.Series([10,9,8,7,6,5,4,3,2,1])\n",
    "rank_series=pd.concat([rating.reset_index(drop=True),new_series],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "4d055ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rating</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.712242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.712242</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          rating         0\n",
       "rating  1.000000  0.712242\n",
       "0       0.712242  1.000000"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_series.corr('spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c135ea",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Here we will compare performence of word2vec and CountVectorizer, and evaluation our system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "48967598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_CV(user_id=64807, title='great teacher onizuka', filename1='anime.csv', filename2='anime_with_synopsis.csv', filename3='rating_complete.csv'):\n",
    "    '''\n",
    "    This function is the main function of our recommender system.\n",
    "    :param user_id: the target user id\n",
    "    :param title: the input anime\n",
    "    :param filename1: data path\n",
    "    :param filename2: data path\n",
    "    :param filename3: data path\n",
    "    :return: top 10 related animes and their 'English name', 'Genres' and 'Rating'\n",
    "    '''\n",
    "    rating_df = read_data(filename1=filename1, filename2=filename2, filename3=filename3)\n",
    "    processed_df = preprocessing(rating_df, user_id=user_id)\n",
    "    \n",
    "    count = CountVectorizer(stop_words='english')\n",
    "    count_matrix = count.fit_transform(processed_df['words_features_tokenized'])\n",
    "    cosine_sim = cosine_similarity(count_matrix, count_matrix)\n",
    "    \n",
    "    name, rating, genre = content_recommender(title=title, cosine_sim=cosine_sim)\n",
    "    \n",
    "    return name, rating, genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "7646a028",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_CV, rating_CV, genre_CV=recommend_CV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "ac39acae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assassination classroom second season \n",
      " 8 \n",
      " action, comedy, school, shounen\n",
      "\n",
      "\n",
      "classroom of the elite \n",
      " 6 \n",
      " slice of life, psychological, drama, school\n",
      "\n",
      "\n",
      "please teacher! \n",
      " 9 \n",
      " sci-fi, comedy, drama, romance, school\n",
      "\n",
      "\n",
      "baka & test - summon the beasts \n",
      " 7 \n",
      " comedy, romance, school, super power\n",
      "\n",
      "\n",
      "sayonara, zetsubou-sensei \n",
      " 9 \n",
      " comedy, parody, school, shounen\n",
      "\n",
      "\n",
      "nobunaga teacher's young bride \n",
      " 3 \n",
      " comedy, ecchi, harem, romance, school\n",
      "\n",
      "\n",
      "azumanga daioh:the animation \n",
      " 8 \n",
      " slice of life, comedy, school\n",
      "\n",
      "\n",
      "hello!! kinmoza! \n",
      " 6 \n",
      " slice of life, comedy, school, seinen\n",
      "\n",
      "\n",
      "hell teacher nube \n",
      " 6 \n",
      " action, supernatural, comedy, school, demons, horror, shounen\n",
      "\n",
      "\n",
      "assassination classroom \n",
      " 7 \n",
      " action, comedy, school, shounen\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(name_CV.iloc[i],\"\\n\", rating_CV.iloc[i],\"\\n\", genre_CV.iloc[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783afe0f",
   "metadata": {},
   "source": [
    "### Compare Diversity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "7c4d55df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get numebr of all genres. we can see there are 45 kinds of genres in total\n",
    "data = read_data()\n",
    "all_genre=set()\n",
    "for i in data['Genres']:\n",
    "    a=word_tokenize(i)\n",
    "    for j in a:\n",
    "        if j == ',':\n",
    "            pass\n",
    "        else:\n",
    "            all_genre.add(j)\n",
    "len(all_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "8aa3e5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['slice of life, comedy, drama, school, shounen']\n",
      "original genres number is  5\n",
      "['psychological', 'supernatural', 'power', 'slice', 'demons', 'super', 'school', 'parody', 'harem', 'shounen', 'action', 'ecchi', 'romance', 'horror', 'seinen', 'sci-fi', 'life', 'drama', 'comedy']\n",
      "current genres length is 19\n"
     ]
    }
   ],
   "source": [
    "#compare original genres and current genres, this is cosine\n",
    "processed_df = preprocessing(data, user_id=64807)\n",
    "original_genre=rating_df[processed_df['English name']=='great teacher onizuka']['Genres']\n",
    "new_genre=set()\n",
    "sentence=list(genre_CV.values)\n",
    "for i in sentence:\n",
    "    a=word_tokenize(i)\n",
    "    for j in a:\n",
    "        if j == ',':\n",
    "            pass\n",
    "        else:\n",
    "            new_genre.add(j)\n",
    "current_genre=[]\n",
    "new_genre=list(new_genre)\n",
    "en_stop=set(stopwords.words('english'))\n",
    "for word in new_genre:\n",
    "    if word not in en_stop:\n",
    "        current_genre.append(word)\n",
    "print(original_genre.values)\n",
    "x=original_genre.values\n",
    "a=x.tolist()\n",
    "a=a[0].split(\",\")\n",
    "print(\"original genres number is \",len(a))\n",
    "print(current_genre)\n",
    "print(\"current genres length is\",len(current_genre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "0ecf3427",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['slice of life, comedy, drama, school, shounen']\n",
      "original genres number is  5\n",
      "['sports', 'psychological', 'power', 'slice', 'shoujo', 'super', 'school', 'adventure', 'ai', 'arts', 'harem', 'shounen', 'action', 'ecchi', 'romance', 'josei', 'game', 'martial', 'life', 'drama', 'comedy']\n",
      "current genres length is 21\n"
     ]
    }
   ],
   "source": [
    "#compare original genres and current genres, this is cosine\n",
    "original_genre=rating_df[processed_df['English name']=='great teacher onizuka']['Genres']\n",
    "new_genre=set()\n",
    "sentence=list(genre.values)\n",
    "for i in sentence:\n",
    "    a=word_tokenize(i)\n",
    "    for j in a:\n",
    "        if j == ',':\n",
    "            pass\n",
    "        else:\n",
    "            new_genre.add(j)\n",
    "current_genre=[]\n",
    "new_genre=list(new_genre)\n",
    "en_stop=set(stopwords.words('english'))\n",
    "for word in new_genre:\n",
    "    if word not in en_stop:\n",
    "        current_genre.append(word)\n",
    "print(original_genre.values)\n",
    "x=original_genre.values\n",
    "a=x.tolist()\n",
    "a=a[0].split(\",\")\n",
    "print(\"original genres number is \",len(a))\n",
    "print(current_genre)\n",
    "print(\"current genres length is\",len(current_genre))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdabf68",
   "metadata": {},
   "source": [
    "### Compare Spearman correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "bd3545e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3742494935411322"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random select five animes\n",
    "name=[]\n",
    "for i in processed_df['English name']:\n",
    "    name.append(i)\n",
    "random.seed(5)\n",
    "name_slice = random.sample(name, 5)\n",
    "rating_list=[]\n",
    "for i in name_slice:\n",
    "    l_name , l_rating, l_genre = recommend(title=i)\n",
    "    rank_series=pd.concat([l_rating.reset_index(drop=True),new_series],axis=1)\n",
    "    rating_list.append(rank_series.corr('spearman')[0]['rating'])\n",
    "mean_value = statistics.mean(rating_list)\n",
    "mean_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "76e47cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14680503798279895"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random select five animes\n",
    "name=[]\n",
    "for i in rating_df['English name']:\n",
    "    name.append(i)\n",
    "random.seed(10)\n",
    "name_slice = random.sample(name, 5)\n",
    "rating_list=[]\n",
    "for i in name_slice:\n",
    "    l_name , l_rating, l_genre =recommend_CV(title=i)\n",
    "    rank_series=pd.concat([l_rating.reset_index(drop=True),new_series],axis=1)\n",
    "    rating_list.append(rank_series.corr('spearman')[0]['rating'])\n",
    "mean_value = statistics.mean(rating_list)\n",
    "mean_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4201b612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
